{"cells":[{"cell_type":"markdown","metadata":{"id":"dplAA4R5mRrE"},"source":["##Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660869356185,"user":{"displayName":"Juliana Thomaz","userId":"10883141159324758422"},"user_tz":180},"id":"OWFK6d3QmRKE"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lucas/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import re\n","import pandas as pd\n","import json\n","import numpy as np\n","import seaborn as sns\n","import nltk\n","from bertopic import BERTopic"]},{"cell_type":"markdown","metadata":{"id":"muyhJsQRVfZH"},"source":["# Extração de Dados"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def df_tweets_candidatos(json_filename):\n","    df = pd.read_json(json_filename).drop(columns = ['replies'])\n","    return df\n","\n","\n","def df_tweets_respostas(json_filename):\n","    df = pd.read_json(json_filename).dropna().reset_index()[['replies']]\n","    ds = []\n","    for replies in df['replies'].to_list():\n","        for reply in replies:\n","            ds.append(reply)\n","\n","    reply_df = pd.DataFrame (ds).drop(columns = ['attachments'])\n","    return reply_df\n","\n","def df_tweets_cadidatos_respostas(json_filename):\n","    df_tweets_cand = df_tweets_candidatos(json_filename)\n","    df_tweets_reps = df_tweets_respostas(json_filename)\n","\n","    return (df_tweets_cand, df_tweets_reps)"]},{"cell_type":"markdown","metadata":{"id":"apymyF3WWswV"},"source":["# Pré-Processamento"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3dHR9A7RWwZU"},"outputs":[{"name":"stdout","output_type":"stream","text":["Número de tweets: 300\n","\n","    author_id      conversation_id                created_at  \\\n","0  2670726740  1560319888643719168 2022-08-18 17:37:05+00:00   \n","1  2670726740  1560299315905515520 2022-08-18 16:15:20+00:00   \n","2  2670726740  1560285152282034176 2022-08-18 15:19:03+00:00   \n","3  2670726740  1560266077329694720 2022-08-18 14:03:15+00:00   \n","4  2670726740  1560248955753095168 2022-08-18 12:55:13+00:00   \n","\n","                                                text                   id  \\\n","0  As pessoas pensam que o sucesso dos nossos gov...  1560319888643719168   \n","1  Já em Belo Horizonte para o primeiro comício d...  1560299315905515520   \n","2  Ontem Lula recebeu a visita de representantes ...  1560285152282034176   \n","3  RT @verdadenarede: Damares condenada! No final...  1560266077329694720   \n","4                      Nove. https://t.co/hfMesT3LHq  1560248955753095168   \n","\n","  candidato  \n","0      lula  \n","1      lula  \n","2      lula  \n","3      lula  \n","4      lula  \n"]}],"source":["tweets = df_tweets_candidatos('./datasets/dataset.json')\n","\n","\n","# Adicionar nomes dos candidatos no dataframe\n","for index, row in tweets.iterrows():\n","    candidato = ''\n","    if row.author_id == 2670726740:\n","      candidato = 'lula'\n","    elif row.author_id == 128372940:\n","      candidato = 'bolsonaro'\n","    elif row.author_id == 33374761:\n","      candidato = 'ciro'\n","    else:\n","      candidato = 'n/d'\n","    tweets.at[index, 'candidato'] = candidato\n","\n","print(f'Número de tweets: {len(tweets)}\\n')\n","print(tweets.head())"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /home/lucas/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["nltk.download('stopwords')\n","pt_stop = set(nltk.corpus.stopwords.words('portuguese'))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import re\n","from nltk.stem import WordNetLemmatizer\n","\n","stemmer = WordNetLemmatizer()\n","\n","def preprocess_text(document):\n","\n","        # removing urls\n","        document = re.sub(r'http\\S+', '', document)\n","\n","        # removing mentions\n","        document = re.sub(r'\\B@\\w+', '', document)\n","\n","        # remove all the special characters\n","        document = re.sub(r'\\W', ' ', str(document))\n","\n","        # remove all single characters\n","        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n","\n","        # remove single characters from the start\n","        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n","\n","        # substituting multiple spaces with single space\n","        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n","\n","        # removing prefixed 'b'\n","        document = re.sub(r'^b\\s+', '', document)\n","\n","        # converting to lowercase\n","        document = document.lower()\n","\n","        # lemmatization\n","        tokens = document.split()\n","        tokens = [stemmer.lemmatize(word) for word in tokens]\n","        tokens = [word for word in tokens if word not in pt_stop]\n","        tokens = [word for word in tokens if len(word) > 3]\n","\n","        return ' '.join(tokens)"]},{"cell_type":"markdown","metadata":{"id":"ZV-mwXjWVoLT"},"source":["# Modelagem de Tópicos"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"VJBsIdssWQC3"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package omw-1.4 to /home/lucas/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package wordnet to /home/lucas/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('omw-1.4')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Batches: 100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n","2022-08-26 23:20:16,954 - BERTopic - Transformed documents to Embeddings\n","2022-08-26 23:20:26,496 - BERTopic - Reduced dimensionality\n","2022-08-26 23:20:26,558 - BERTopic - Clustered reduced embeddings\n"]}],"source":["# create model \n","model = BERTopic(language=\"multilingual\", verbose=True, min_topic_size=3, top_n_words=10, calculate_probabilities=True)\n","\n","# convert to list \n","docs = tweets.text.to_list()\n","\n","# Pre-processamento\n","for i, doc in enumerate(docs):\n","    docs[i] = preprocess_text(doc)\n","\n","tweets_topic, probabilities = model.fit_transform(docs)"]},{"cell_type":"markdown","metadata":{},"source":["Adicionando o tópico de cada tweet no Dataframe"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["topic_names = model.generate_topic_labels(nr_words=4)\n","\n","# Array indicando o tópico de cada tweet (numérico -> índice do array \"topic_names\" + 1)\n","# print(tweets_topic)\n","# Array com todos os tópicos nomeados\n","# print(topic_names)\n","\n","# print(docs)\n","\n","\n","for index, tweet_topic in enumerate(tweets_topic):\n","    tweets.at[index, 'topico'] = topic_names[tweet_topic+1]\n","    tweets.at[index, 'processed_text'] = docs[index]\n","\n","# DEBUG\n","# display(tweets)\n","# tweets.to_csv('/home/rodrigo98rm/Documents/ufabc/pln/projeto-2/pln-2022.2/src/output/dataframe.csv', sep='\\t', encoding='utf-8')"]},{"cell_type":"markdown","metadata":{},"source":["Abaixo, os tópicos identificados entre todos os tweets"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["    Topic  Count                                         Name\n","0      -1     43               -1_ciro_brasil_produz_economia\n","1       0     25                 0_luladay_noite_gettr_baixar\n","2       1     25  1_esperança_melhor_brasildaesperança_brasil\n","3       2     16                        2_2022_jair_aula_giro\n","4       3     16                   3_brasil_país_vamos_voltar\n","5       4     16              4_custou_pequenos_básico_quanto\n","6       5     14         5_redução_energia_combustíveis_preço\n","7       6     11           6_carta_saudades_democracia_drogas\n","8       7     11                7_milhões_famílias_safra_fome\n","9       8     11                 8_deus_cristãos_irmão_adorar\n","10      9     10                    9_eleição_voto_falar_café\n","11     10     10      10_link_veio_repare_cirocomandrémarinho\n","12     11      9                 11_torcida_todos_amanhã_vivo\n","13     12      8           12_jovens_estímulo_prática_decreto\n","14     13      8     13_central_pergunta_cironorodaviva_banco\n","15     14      7          14_programa_renda_mínima_distribuir\n","16     15      7               15_prefeito_rota_falando_junto\n","17     16      6            16_campanha_oficial_convido_limpa\n","18     17      6          17_corrupção_chamar_errado_história\n","19     18      6               18_juros_entregou_maior_guerra\n","20     19      6       19_paixões_ódios_despolitizadas_social\n","21     20      5          20_garantido_governo_programa_renda\n","22     21      5        21_auxílio_emergencial_dezembro_agora\n","23     22      5         22_entenda_maconha_drogas_organizado\n","24     23      4           23_canal_vivo_acompanhe_entrevista\n","25     24      4                24_abraço_família_herói_percy\n","26     25      3  25_universidade_oportunidade_iniciar_atrair\n","27     26      3               26_crime_luta_acreditem_eterna\n"]}],"source":["freq = model.get_topic_info()\n","print(freq)"]},{"cell_type":"markdown","metadata":{"id":"HooJDbp4WERL"},"source":["# Análise de Sentimentos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANENPKpmWQeE"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wYl-0IqqWI6u"},"source":["# Similaridade de Textos"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"5LANdfj4WRBM"},"outputs":[],"source":["def monta_docs_tweets(df, topico, coluna_alvo, candidatos):\n","    lista_tweets = []\n","    #df_topico = df[df['topico'] == topico]\n","    df_topico = df\n","\n","    for cand in candidatos:\n","        df_cand = df_topico[df_topico['candidato'] == cand]\n","        lista_tweets_cand = df_cand[coluna_alvo].to_list()\n","        lista_tweets.append(lista_tweets_cand)\n","    \n","    docs = np.concatenate(lista_tweets)\n","    return docs\n","\n","def monta_docs_concat_topico(df, topico, coluna_alvo, candidatos):\n","    docs = []\n","    df_topico = df[(df['topico'] == topico)]\n","    for cand in candidatos:\n","        df_topico_cand = df_topico[df_topico['candidato'] == cand]\n","        texto = ' '.join(df_topico_cand[coluna_alvo].to_list())\n","        docs.append(texto)\n","    return docs\n","\n","\n","def monta_docs_concat_candidato(df, coluna_alvo, candidatos):\n","    docs = []\n","    for cand in candidatos:\n","        df_cand = df[df['candidato'] == cand]\n","        texto = ' '.join(df_cand[coluna_alvo].to_list())\n","        docs.append(texto)\n","    return docs\n","\n","stop_words_ptbr = nltk.corpus.stopwords.words('portuguese')\n","\n","#A funcao normaliza_tweet não foi necessária, decidimos reutilizar o pré processamento já realizado na modelagem de tópicos\n","def normaliza_tweet(tweet):\n","    tweet = re.sub(r'[^a-zA-Z0-9\\s]', '', tweet, re.I|re.A)\n","    tweet = tweet.lower()\n","    tweet = tweet.strip()\n","    tokens = nltk.word_tokenize(tweet)\n","    tokens_filtrados = [token for token in tokens if token not in stop_words_ptbr]\n","    tweet_tokens = ' '.join(tokens_filtrados)\n","    return tweet_tokens\n","\n","\n","def gera_matriz_tfidf(corpus_vetorizado):\n","    tf = TfidfVectorizer(ngram_range=(1, 2), min_df=1)\n","    matriz_tfidf = tf.fit_transform(corpus_vetorizado)\n","    return matriz_tfidf\n","\n","def similaridade_pares(matriz_tfidf):\n","    dict_docs_sim = cosine_similarity(matriz_tfidf)\n","    df_sim = pd.DataFrame(dict_docs_sim)\n","    return df_sim\n","\n","def renomear_colunas_linhas(df, candidatos):\n","    df.columns = candidatos\n","    df.index = candidatos\n","    return df\n","\n","def calcula_similaridade_topico(df, topico, coluna_alvo, candidatos):\n","    lista_docs = monta_docs_concat_topico(df, topico, coluna_alvo, candidatos)\n","    corpus_normalizado = lista_docs\n","    matriz_tfidf = gera_matriz_tfidf(corpus_normalizado)\n","    df_sim = similaridade_pares(matriz_tfidf)\n","    df_sim_cands = renomear_colunas_linhas(df_sim, candidatos)\n","\n","    return df_sim_cands\n","\n","def calcula_similaridade_candidatos(df, coluna_alvo, candidatos):\n","    lista_docs = monta_docs_concat_candidato(df, coluna_alvo, candidatos)\n","    corpus_normalizado = lista_docs\n","    matriz_tfidf = gera_matriz_tfidf(corpus_normalizado)\n","    df_sim = similaridade_pares(matriz_tfidf)\n","    df_sim_cands = renomear_colunas_linhas(df_sim, candidatos)\n","\n","    return df_sim\n","\n","def calcula_similaridade_topicos(df, topicos, coluna_alvo, candidatos):\n","    dfs_sim = []\n","    for topico in topicos:\n","        dfs_sim.append(calcula_similaridade_topico(df, topico, coluna_alvo, candidatos))\n","        \n","    sim_dict = dict(zip(topicos, dfs_sim))\n","    return sim_dict"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["['-1_ciro_brasil_produz_economia',\n"," '0_luladay_noite_gettr_baixar',\n"," '1_esperança_melhor_brasildaesperança_brasil',\n"," '2_2022_jair_aula_giro']"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df_freq = model.get_topic_info()\n","topicos_mais_frequentes = df_freq['Name'].to_list()[0:4]\n","topicos_mais_frequentes"]},{"cell_type":"markdown","metadata":{},"source":["**Comparando Similaridade entre os candidatos para os tópicos mais frequentes e utilizando o texto processado já na fase de modelagem**"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Para tópico = -1_ciro_brasil_produz_economia\n","               lula  bolsonaro      ciro\n","lula       1.000000    0.04225  0.048292\n","bolsonaro  0.042250    1.00000  0.023550\n","ciro       0.048292    0.02355  1.000000\n","----------------------------------\n","Para tópico = 0_luladay_noite_gettr_baixar\n","               lula  bolsonaro  ciro\n","lula       1.000000   0.029581   0.0\n","bolsonaro  0.029581   1.000000   0.0\n","ciro       0.000000   0.000000   1.0\n","----------------------------------\n","Para tópico = 1_esperança_melhor_brasildaesperança_brasil\n","               lula  bolsonaro      ciro\n","lula       1.000000   0.025601  0.065461\n","bolsonaro  0.025601   1.000000  0.031014\n","ciro       0.065461   0.031014  1.000000\n","----------------------------------\n","Para tópico = 2_2022_jair_aula_giro\n","               lula  bolsonaro      ciro\n","lula       1.000000   0.000000  0.008196\n","bolsonaro  0.000000   1.000000  0.120712\n","ciro       0.008196   0.120712  1.000000\n","----------------------------------\n"]}],"source":["candidatos = [\"lula\", \"bolsonaro\", \"ciro\"]\n","analise_similaridade = calcula_similaridade_topicos(tweets, topicos_mais_frequentes, \"processed_text\", candidatos)\n","for topico in analise_similaridade.keys():\n","    print(f\"Para tópico = {topico}\")\n","    print(analise_similaridade[topico])\n","    print('----------------------------------')"]},{"cell_type":"markdown","metadata":{},"source":["**Comparando Similaridade entre os candidatos entre todos os tweets (ou seja, todos os tópicos) e utilizando o texto processado já na fase de modelagem**"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lula</th>\n","      <th>bolsonaro</th>\n","      <th>ciro</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>lula</th>\n","      <td>1.000000</td>\n","      <td>0.147990</td>\n","      <td>0.209275</td>\n","    </tr>\n","    <tr>\n","      <th>bolsonaro</th>\n","      <td>0.147990</td>\n","      <td>1.000000</td>\n","      <td>0.115364</td>\n","    </tr>\n","    <tr>\n","      <th>ciro</th>\n","      <td>0.209275</td>\n","      <td>0.115364</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               lula  bolsonaro      ciro\n","lula       1.000000   0.147990  0.209275\n","bolsonaro  0.147990   1.000000  0.115364\n","ciro       0.209275   0.115364  1.000000"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["analise_sim_cand = calcula_similaridade_candidatos(tweets, \"processed_text\", candidatos)\n","analise_sim_cand"]},{"cell_type":"markdown","metadata":{},"source":["Aqui, em nossa primeira tentativa decidimos usar diretamente os **textos normalizados** e obtivemos uma baixa similaridade entre os candidatos, tanto no escopo por tópico quanto entre todos os tópicos, o que foi de acordo com nossa hipótese, dado que são 3 candidatos com percepções bem distintas entre os 3, então tivemos um resultado bem satisfatório, obtendo similaridades menores que 10% entre os candidatos para cada um dos tópicos, e obtendo similaridades menores que 21% entre os candidatos ao olhar todos os tópicos.\n","\n","Além disso, a maior similaridade se dá entre Lula e Ciro, que também era esperado por nosso grupo dada o discurso praticamente antagônico entre Lula e Bolsonaro."]},{"cell_type":"markdown","metadata":{},"source":["**Comparando Similaridade entre os candidatos para os tópicos mais frequentes e utilizando o texto não-processado já na fase de modelagem**"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Para tópico = -1_ciro_brasil_produz_economia\n","               lula  bolsonaro      ciro\n","lula       1.000000   0.292067  0.190836\n","bolsonaro  0.292067   1.000000  0.259823\n","ciro       0.190836   0.259823  1.000000\n","----------------------------------\n","Para tópico = 0_luladay_noite_gettr_baixar\n","               lula  bolsonaro      ciro\n","lula       1.000000   0.539863  0.193510\n","bolsonaro  0.539863   1.000000  0.256996\n","ciro       0.193510   0.256996  1.000000\n","----------------------------------\n","Para tópico = 1_esperança_melhor_brasildaesperança_brasil\n","               lula  bolsonaro      ciro\n","lula       1.000000   0.149390  0.167282\n","bolsonaro  0.149390   1.000000  0.146576\n","ciro       0.167282   0.146576  1.000000\n","----------------------------------\n","Para tópico = 2_2022_jair_aula_giro\n","               lula  bolsonaro      ciro\n","lula       1.000000   0.116306  0.106106\n","bolsonaro  0.116306   1.000000  0.358978\n","ciro       0.106106   0.358978  1.000000\n","----------------------------------\n"]}],"source":["candidatos = [\"lula\", \"bolsonaro\", \"ciro\"]\n","analise_similaridade = calcula_similaridade_topicos(tweets, topicos_mais_frequentes, \"text\", candidatos)\n","for topico in analise_similaridade.keys():\n","    print(f\"Para tópico = {topico}\")\n","    print(analise_similaridade[topico])\n","    print('----------------------------------')"]},{"cell_type":"markdown","metadata":{},"source":["**Comparando Similaridade entre os candidatos entre todos os tweets (ou seja, todos os tópicos) e utilizando o texto não-processado já na fase de modelagem**"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lula</th>\n","      <th>bolsonaro</th>\n","      <th>ciro</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>lula</th>\n","      <td>1.000000</td>\n","      <td>0.636385</td>\n","      <td>0.647811</td>\n","    </tr>\n","    <tr>\n","      <th>bolsonaro</th>\n","      <td>0.636385</td>\n","      <td>1.000000</td>\n","      <td>0.666628</td>\n","    </tr>\n","    <tr>\n","      <th>ciro</th>\n","      <td>0.647811</td>\n","      <td>0.666628</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               lula  bolsonaro      ciro\n","lula       1.000000   0.636385  0.647811\n","bolsonaro  0.636385   1.000000  0.666628\n","ciro       0.647811   0.666628  1.000000"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["analise_sim_cand = calcula_similaridade_candidatos(tweets, \"text\", candidatos)\n","analise_sim_cand"]},{"cell_type":"markdown","metadata":{},"source":["Nesta segunda tentativa tentamos utilizar o texto não normalizado e tivemos similaridades maiores em todos os contextos testados, sendo isso provavelmente ocasionado pela ocorrência de stop-words que estejam enviesando nosso resultado (visto que elas aparecem em grande em um número em um texto). No entanto, é possível perceber que a relação de maiores e menores similaridades entre os candidates persistem mesmo para a versão não normalizada."]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Projeto2-PLN.ipynb","provenance":[]},"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}
