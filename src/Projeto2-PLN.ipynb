{"cells":[{"cell_type":"markdown","metadata":{"id":"dplAA4R5mRrE"},"source":["##Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660869356185,"user":{"displayName":"Juliana Thomaz","userId":"10883141159324758422"},"user_tz":180},"id":"OWFK6d3QmRKE"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /home/lucas/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /home/lucas/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","import pandas as pd\n","import json\n","import nltk\n","import numpy as np\n","\n","#Imports similaridade\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"id":"muyhJsQRVfZH"},"source":["# Extração de Dados"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def df_tweets_candidatos(json_filename):\n","    df = pd.read_json(json_filename).drop(columns = ['replies'])\n","    return df\n","\n","\n","def df_tweets_respostas(json_filename):\n","    df = pd.read_json(json_filename).dropna().reset_index()[['replies']]\n","    ds = []\n","    for replies in df['replies'].to_list():\n","        for reply in replies:\n","            ds.append(reply)\n","\n","    reply_df = pd.DataFrame (ds).drop(columns = ['attachments'])\n","    return reply_df\n","\n","def df_tweets_cadidatos_respostas(json_filename):\n","    df_tweets_cand = df_tweets_candidatos(json_filename)\n","    df_tweets_reps = df_tweets_respostas(json_filename)\n","\n","    return (df_tweets_cand, df_tweets_reps)"]},{"cell_type":"markdown","metadata":{"id":"apymyF3WWswV"},"source":["# Pré-Processamento"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"3dHR9A7RWwZU"},"outputs":[],"source":["def normaliza(df, coluna_alvo='text'):\n","  coluna_normal = coluna_alvo+\"_normal\"\n","  df[coluna_normal] = df[coluna_alvo].str.lower()\n","  df[coluna_normal].replace(to_replace = '[^a-zA-ZÀ-ÖØ-öø-ÿ\\s]', value='', regex = True, inplace = True)\n","  return df\n","  \n","def tokeniza(df, coluna_alvo='text_normal'):\n","  stop_words = nltk.corpus.stopwords.words('portuguese')\n","  todos_tokens = []\n","  todos_textos_tokens = []\n","\n","  for texto_normal in df[coluna_alvo].to_list():\n","    tokens = nltk.word_tokenize(texto_normal, language=\"portuguese\")\n","    tokens_filtrados = [token for token in tokens if token not in stop_words]\n","    todos_tokens.append(tokens_filtrados)\n","    texto_tokens = ' '.join(tokens_filtrados)\n","    todos_textos_tokens.append(texto_tokens)\n","\n","\n","  df_tokens = pd.DataFrame({'tokens': todos_tokens, 'texto_tokens': todos_textos_tokens})\n","  df_concat = pd.concat([df, df_tokens], axis=\"columns\")\n","\n","  return df_concat\n","\n","\n","def normaliza_tokeniza(df, coluna_alvo = 'text'):\n","  df_tokens = tokeniza(normaliza(df, coluna_alvo), coluna_alvo)\n","  return df_tokens\n","\n","  #word_regex = r\"[-'a-zA-ZÀ-ÖØ-öø-ÿ]+\" #para capturarmos palavras dentro do tweet\n","  #df['text'] = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', df['text'], flags=re.MULTILINE)  #para remover os links do campo de texto do tweet"]},{"cell_type":"markdown","metadata":{"id":"ZV-mwXjWVoLT"},"source":["# Modelagem de Tópicos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJBsIdssWQC3"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"HooJDbp4WERL"},"source":["*texto em itálico*# Análise de Sentimentos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANENPKpmWQeE"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wYl-0IqqWI6u"},"source":["# Similaridade de Textos"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"5LANdfj4WRBM"},"outputs":[],"source":["def monta_docs_tweets(df, coluna_alvo, candidatos):\n","    lista_tweets = []\n","    for cand in candidatos:\n","        df_cand = df[df['candidato'] == cand]\n","        lista_tweets_cand = df_cand[coluna_alvo].to_list()\n","        lista_tweets.append(lista_tweets_cand)\n","    \n","    docs = np.concatenate(lista_tweets)\n","    return docs\n","\n","def monta_docs_concat(df, topico, coluna_alvo, candidatos):\n","    docs = []\n","    for cand in candidatos:\n","        df_filtrado = df[df['topico'] == topico and df['candidato'] == cand]\n","        texto = ' '.join(df_filtrado[coluna_alvo].to_list())\n","        docs.append(texto)\n"," \n","    return docs\n","\n","def vetoriza_docs(lista_docs):\n","    corpus_vetorizado = np.vectorize(lista_docs)\n","    return corpus_vetorizado\n","\n","def gera_matriz_tfidf(corpus_vetorizado):\n","    tf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n","    matriz_tfidf = tf.fit_transform(corpus_vetorizado)\n","    return matriz_tfidf\n","\n","def similaridade_pares(matriz_tfidf):\n","    dict_docs_sim = cosine_similarity(matriz_tfidf)\n","    df_sim = pd.DataFrame(dict_docs_sim)\n","    return df_sim\n","\n","def calcula_similaridade(df, topico, coluna_alvo, candidatos):\n","    lista_docs = monta_docs_concat(df, topico, coluna_alvo, candidatos)\n","    corpus_vetorizado = vetoriza_docs(lista_docs)\n","    matriz_tfidf = gera_matriz_tfidf(corpus_vetorizado)\n","    df_sim = similaridade_pares(matriz_tfidf)\n","\n","    return df_sim"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Projeto2-PLN.ipynb","provenance":[]},"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}
